{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricted Botlzman Machines (RBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME: Review the generation process (theoretically) and fix the implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Tuple, Literal, Optional, Union, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "from numpy._typing import ArrayLike\n",
    "\n",
    "ArrayLike = Union[List, Tuple, np.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../data/\"\n",
    "ALPHA_DIGIT_PATH = os.path.join(DATA_FOLDER, \"binaryalphadigs.mat\")\n",
    "MNIST_PATH = os.path.join(DATA_FOLDER, \"mnist_all.mat\")\n",
    "\n",
    "if not os.path.exists(ALPHA_DIGIT_PATH):\n",
    "    raise FileNotFoundError(f\"The file {ALPHA_DIGIT_PATH} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Implementing a RBM and testing on Binary AlphaDigits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(file_path: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load Binary AlphaDigits data from a .mat file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the .mat file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - data (dict): Loaded data dictionary.\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(\"File path must be provided.\")\n",
    "\n",
    "    return scipy.io.loadmat(file_path)\n",
    "\n",
    "\n",
    "data = _load_data(ALPHA_DIGIT_PATH)\n",
    "class_labels = data[\"classlabels\"].flatten() \n",
    "class_count = data[\"classcounts\"].flatten()\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class Labels\": class_labels,\n",
    "        \"Class Count\": class_count\n",
    "    }\n",
    ")\n",
    "df[\"Class Labels\"] = df[\"Class Labels\"].apply(lambda x: x[0])\n",
    "df[\"Class Count\"] = df[\"Class Count\"].apply(lambda x: x[0][0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(file_path: str, which: Literal[\"alphadigit\", \"mnist\"]=\"alphadigit\") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load Binary AlphaDigits data from a .mat file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the .mat file containing the data.\n",
    "    - which (Literal[\"alphadigit\", \"mnist\"], optional): Specifies \n",
    "        which data to load. The default value is \"alphadigit\".\n",
    "\n",
    "    Returns:\n",
    "    - data (dict): A dictionary containing the loaded data.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the file_path parameter is None.\n",
    "    - ValueError: If the which parameter is not \"alphadigit\".\n",
    "\n",
    "    Example Usage:\n",
    "    ```python\n",
    "    data = _load_data(\"data.mat\", \"alphadigit\")\n",
    "    ```\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(\"File path must be provided.\")\n",
    "    \n",
    "    if which == \"alphadigit\":\n",
    "        return scipy.io.loadmat(file_path)[\"dat\"]\n",
    "    \n",
    "    raise ValueError(\"MNIST NOT YET AVAILABLE.\")\n",
    "\n",
    "alphadigit_data = _load_data(ALPHA_DIGIT_PATH) \n",
    "print(alphadigit_data.shape)\n",
    "print(alphadigit_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_characters_to_indices(characters: Union[str, int, List[Union[str, int]]]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Map alphanumeric character to its corresponding index.\n",
    "\n",
    "    Parameters:\n",
    "    - character (str, int, list of str or int): Alphanumeric character or its index.\n",
    "\n",
    "    Returns:\n",
    "    - char_index (int): Corresponding index for the character.\n",
    "    \"\"\"\n",
    "    if isinstance(characters, list):\n",
    "        return [_map_characters_to_indices(char) for char in characters]\n",
    "    if isinstance(characters, int) and 0 <= characters <= 35:\n",
    "        return [characters]\n",
    "    if (isinstance(characters, str) and characters.isdigit()\n",
    "          and 0 <= int(characters) <= 9):\n",
    "        return [int(characters)]\n",
    "    if (isinstance(characters, str) and characters.isalpha()\n",
    "          and 'A' <= characters.upper() <= 'Z'):\n",
    "        return [ord(characters.upper()) - ord('A') + 10]\n",
    "    \n",
    "    raise ValueError(\n",
    "        \"Invalid character input. It should be an alphanumeric\" \n",
    "        \"character '[0-9|A-Z]' or its index representing '[0-35]'.\"\n",
    "    )\n",
    "\n",
    "for char in [0, 10, \"A\", [1, \"C\"], 36]:\n",
    "    try:\n",
    "        map = _map_characters_to_indices(char)\n",
    "        print(f\"{char} > map to > {map}\")\n",
    "    except:\n",
    "        print(f\"{char} > no mapping available, out of range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_alpha_digit(characters: Optional[Union[str, int, List[Union[str, int]]]] = None,\n",
    "                     file_path: Optional[str] = ALPHA_DIGIT_PATH,\n",
    "                     data: Optional[Dict[str, np.ndarray]] = None,\n",
    "                     use_data: bool = False,\n",
    "                     ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads binary AlphaDigits data from a .mat file or uses already loaded data. \n",
    "    It extracts the data for a specified alphanumeric character or its index, and \n",
    "    flattens the images into one-dimensional vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - characters (Union[str, int, List[Union[str, int]]], optional): Alphanumeric character \n",
    "        or its index whose data needs to be extracted. It can be a single character or \n",
    "        a list of characters. Default is None.\n",
    "    - file_path (str, optional): Path to the .mat file containing the data. \n",
    "        Default is None.\n",
    "    - data (dict, optional): Already loaded data dictionary. \n",
    "        Default is None.\n",
    "    - use_data (bool): Flag to indicate whether to use already loaded data.\n",
    "        Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - flattened_images (numpy.ndarray): Flattened images for the specified character(s).\n",
    "    \"\"\"\n",
    "    if not use_data:\n",
    "        data = _load_data(file_path, which=\"alphadigit\")\n",
    "\n",
    "    char_indices = _map_characters_to_indices(characters)\n",
    "\n",
    "    # Select the rows corresponding to the characters indices.\n",
    "    char_data: np.ndarray = data[char_indices]\n",
    "    \n",
    "    # Flatten each image into a one-dimensional vector.\n",
    "    flattened_images = np.array([image.flatten() for image in char_data.flatten()])\n",
    "    return flattened_images\n",
    "\n",
    "def plot_characters(chars, data):\n",
    "    num_chars = len(chars)\n",
    "    num_images_per_char = data.shape[0] // num_chars\n",
    "    fig, ax = plt.subplots(1, num_chars, figsize=(num_chars * 2, 2))\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        # Find the index of the first image corresponding to the current char\n",
    "        start_index = i * num_images_per_char\n",
    "        image = data[start_index].reshape(20, 16)\n",
    "        ax[i].imshow(image, cmap='gray')\n",
    "        ax[i].set_title(f'Char: {char}')\n",
    "        ax[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "chars = [0, \"K\", 7, \"Z\"]\n",
    "data = read_alpha_digit(chars, data=alphadigit_data, use_data=True)\n",
    "plot_characters(chars, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, n_visible: int, n_hidden: int=100, random_state=None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Restricted Boltzmann Machine.\n",
    "\n",
    "        Parameters:\n",
    "        - n_visible (int): Number of visible units.\n",
    "        - n_hidden (int): Number of hidden units. Default 100.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.a = np.zeros((1, n_visible)) # visible_bias\n",
    "        self.b = np.zeros((1, n_hidden)) # hidden_bias\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "        self.W = 1e-4 * self.rng.standard_normal(size=(n_visible, n_hidden)) # weights\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"RBM(n_visible={self.n_visible}, n_hidden={self.n_hidden})\"\n",
    "\n",
    "    def _sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x (numpy.ndarray): Input array.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Result of applying the sigmoid function to the input.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _reconstruction_error(self, input: np.ndarray, image: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute reconstruction error.\n",
    "\n",
    "        Parameters:\n",
    "        - input (numpy.ndarray): Original input data.\n",
    "        - image (numpy.ndarray): Reconstructed image.\n",
    "\n",
    "        Returns:\n",
    "        - float: Reconstruction error.\n",
    "        \"\"\"\n",
    "        return np.round(np.power(image - input, 2).mean(), 5)\n",
    "\n",
    "    def input_output(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute hidden units given visible units.\n",
    "\n",
    "        Parameters:\n",
    "        - data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Hidden unit activations, shape (n_samples, n_hidden).\n",
    "        \"\"\"\n",
    "        return self._sigmoid(data @ self.W + self.b)\n",
    "\n",
    "    def output_input(self, data_h: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute visible units given hidden units.\n",
    "\n",
    "        Parameters:\n",
    "        - data_h (numpy.ndarray): Hidden unit activations, shape (n_samples, n_hidden).\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Reconstructed visible units, shape (n_samples, n_visible).\n",
    "        \"\"\"\n",
    "        return self._sigmoid(data_h @ self.W.T + self.a)\n",
    "    \n",
    "    def calcul_softmax(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate softmax probabilities for the output units.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Softmax probabilities, shape (n_samples, n_hidden).\n",
    "        \"\"\"\n",
    "        # Compute activations for the hidden layer\n",
    "        hidden_activations = self.input_output(data)\n",
    "        \n",
    "        # Compute softmax probabilities for the output layer\n",
    "        exp_hidden_activations = np.exp(hidden_activations)\n",
    "        softmax_probs = exp_hidden_activations / np.sum(exp_hidden_activations, axis=1, keepdims=True)\n",
    "        \n",
    "        return softmax_probs\n",
    "\n",
    "    def update(\n",
    "            self, \n",
    "            batch: np.ndarray,\n",
    "            learning_rate: float=0.1,\n",
    "            batch_size: Optional[int]=None,\n",
    "            return_output: bool=False\n",
    "        ):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            batch (np.ndarray): _description_\n",
    "            learning_rate (float, optional): _description_. Defaults to 0.1.\n",
    "            batch_size (Optional[int], optional): _description_. Defaults to None.\n",
    "            return_output (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        if not batch_size:\n",
    "            batch_size = batch.shape[0]\n",
    "        pos_h_probs = self.input_output(batch)\n",
    "        pos_v_probs = self.output_input(pos_h_probs)\n",
    "        neg_h_probs = self.input_output(pos_v_probs)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.W += learning_rate * (batch.T @ pos_h_probs - pos_v_probs.T @ neg_h_probs) / batch_size\n",
    "        self.b += learning_rate * (pos_h_probs - neg_h_probs).mean(axis=0)\n",
    "        self.a += learning_rate * (batch - pos_v_probs).mean(axis=0)\n",
    "\n",
    "        if return_output:\n",
    "            return self, pos_v_probs\n",
    "        \n",
    "        return self \n",
    "\n",
    "    def train(self, \n",
    "              data: np.ndarray,\n",
    "              learning_rate: float=0.1,\n",
    "              n_epochs: int=10,\n",
    "              batch_size: int=10,\n",
    "              print_each=10\n",
    "        ) -> 'RBM':\n",
    "        \"\"\"\n",
    "        Train the RBM using Contrastive Divergence.\n",
    "\n",
    "        Parameters:\n",
    "        - data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "        - learning_rate (float): Learning rate for gradient descent. Default is 0.1.\n",
    "        - n_epochs (int): Number of training epochs. Default is 10.\n",
    "        - batch_size (int): Size of mini-batches. Default is 10.\n",
    "\n",
    "        Returns:\n",
    "        - RBM: Trained RBM instance.\n",
    "        \"\"\"\n",
    "        n_samples = data.shape[0]\n",
    "        for epoch in range(n_epochs):\n",
    "            self.rng.shuffle(data)\n",
    "            for i in tqdm(range(0, n_samples, batch_size), desc=f\"Epoch {epoch}\"):\n",
    "                batch = data[i:i+batch_size]\n",
    "                _, pos_v_probs = self.update(\n",
    "                    batch=batch,\n",
    "                    learning_rate=learning_rate,\n",
    "                    batch_size=batch_size,\n",
    "                    return_output=True\n",
    "                )\n",
    "                \n",
    "            if epoch % print_each == 0:\n",
    "                tqdm.write(\n",
    "                    f\"Reconstruction error: {self._reconstruction_error(batch, pos_v_probs)}.\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate_image(self, n_samples: int=1, n_gibbs_steps: int=1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate samples from the RBM using Gibbs sampling.\n",
    "\n",
    "        Parameters:\n",
    "        - n_samples (int): Number of samples to generate. Default is 10.\n",
    "        - n_gibbs_steps (int): Number of Gibbs sampling steps. Default is 1.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Generated samples, shape (n_samples, n_visible).\n",
    "        \"\"\"\n",
    "        samples = np.zeros((n_samples, self.n_visible))\n",
    "        \n",
    "        # Matrix of initlization value of Gibbs samples for each sample. \n",
    "        V = self.rng.binomial(1, self.rng.random(), size=n_samples*self.n_visible).reshape((n_samples, self.n_visible))\n",
    "        for i in range(n_samples):\n",
    "            for _ in range(n_gibbs_steps):\n",
    "                h_probs = self._sigmoid(V[i] @ self.W + self.b) # vector\n",
    "                h = self.rng.binomial(1, h_probs)\n",
    "                v_probs = self._sigmoid(h @ self.W.T + self.a)\n",
    "                v = self.rng.binomial(1, v_probs)\n",
    "            samples[i] = v\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the alpha_digit data\n",
    "data = read_alpha_digit(file_path=ALPHA_DIGIT_PATH, characters=['Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_visible = data.shape[1]  # Number of visible units (size of each image)\n",
    "n_hidden = 200  # Number of hidden units (hyperparameter)\n",
    "\n",
    "# Initialize RBM\n",
    "rbm = RBM(n_visible=n_visible, n_hidden=n_hidden, random_state=42)\n",
    "print(rbm)\n",
    "\n",
    "# Train RBM\n",
    "rbm.train(data, learning_rate=0.1, n_epochs=500, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(rbm.calcul_softmax(data).sum(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples\n",
    "generated_samples = rbm.generate_image(n_samples=10, n_gibbs_steps=1)\n",
    "\n",
    "# Plot original and generated samples\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 10, i + 1)\n",
    "    plt.imshow(data[i].reshape(20, 16), cmap='gray')\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 10, i + 11)\n",
    "    plt.imshow(generated_samples[i].reshape(20, 16), cmap='gray')\n",
    "    plt.title('Generated')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementing a Deep Belief Network (DBN) and test on Binary AlphaDigits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN:\n",
    "    def __init__(self, n_visible: int, hidden_layer_sizes: list[int], random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize the Deep Belief Network.\n",
    "\n",
    "        Parameters:\n",
    "        - n_visible (int): Number of visible units.\n",
    "        - hidden_layer_sizes (list[int]): List of sizes for each hidden layer.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.rbms: List[RBM] = []\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "        # Initialize the first RBM\n",
    "        first_rbm = RBM(\n",
    "            n_visible=n_visible,\n",
    "            n_hidden=hidden_layer_sizes[0],\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        self.rbms.append(first_rbm)\n",
    "\n",
    "        # Initialize RBMs for subsequent hidden layers\n",
    "        for i, size in enumerate(hidden_layer_sizes[1:], start=1):\n",
    "            rbm = RBM(\n",
    "                n_visible=hidden_layer_sizes[i - 1],\n",
    "                n_hidden=size,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "            self.rbms.append(rbm)\n",
    "\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.rbms[key]\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a string representation of the DBN object.\n",
    "        \"\"\"\n",
    "        rbm_reprs = [f\"{'':4}{repr(rbm)}\" for rbm in self.rbms]\n",
    "        join_rbm_reprs = ',\\n'.join(rbm_reprs)\n",
    "        return f\"DBN([\\n{join_rbm_reprs}\\n])\"\n",
    "\n",
    "\n",
    "    def train(self,\n",
    "        data: np.ndarray,\n",
    "        learning_rate: float=0.1,\n",
    "        n_epochs: int=10,\n",
    "        batch_size: int=10,\n",
    "        print_each: int=10,\n",
    "    ) -> \"DBN\":\n",
    "        \"\"\"\n",
    "        Train the DBN using Greedy layer-wise procedure.\n",
    "\n",
    "        Parameters:\n",
    "        - data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "        - learning_rate (float): Learning rate for gradient descent. Default is 0.1.\n",
    "        - n_epochs (int): Number of training epochs. Default is 10.\n",
    "        - batch_size (int): Size of mini-batches. Default is 10.\n",
    "        - print_each: Print reconstruction error each `print_each` epochs.\n",
    "        - verbose\n",
    "\n",
    "        Returns:\n",
    "        - DBN: Trained DBN instance.\n",
    "        \"\"\"\n",
    "        input_data = data\n",
    "        for rbm in self.rbms:\n",
    "            rbm.train(\n",
    "                input_data,\n",
    "                learning_rate=learning_rate,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=batch_size,\n",
    "                print_each=print_each,\n",
    "            )\n",
    "            # Update input data for the next RBM\n",
    "            input_data = rbm.input_output(input_data)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def generate_image(self, n_samples: int=1, n_gibbs_steps: int=1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate samples from the DBN using Gibbs sampling.\n",
    "\n",
    "        Parameters:\n",
    "        - n_samples (int): Number of samples to generate. Default is 1.\n",
    "        - n_gibbs_steps (int): Number of Gibbs sampling steps. Default is 100.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Generated samples, shape (n_samples, n_visible).\n",
    "        \"\"\"\n",
    "        # samples = np.zeros((n_samples, self.n_visible))\n",
    "\n",
    "        # Generate samples using the first RBM in the DBN\n",
    "        samples = self.rbms[-1].generate_image(n_samples, n_gibbs_steps)\n",
    "        for rbm in reversed(self.rbms[:-1]):\n",
    "            # Sample from the conditional probability of layer l-1 given layer l: p(h_{s-1}|h_{s}).\n",
    "            h_probs = rbm.output_input(samples)\n",
    "            h = self.rng.binomial(1, p=h_probs) \n",
    "            samples = h\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from principal_dbn_alpha import DBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_visible=data.shape[1]\n",
    "hidden_layer_sizes = [100, 50, 25]\n",
    "\n",
    "dbn = DBN(n_visible=n_visible, hidden_layer_sizes=hidden_layer_sizes, random_state=42)\n",
    "dbn.train(data, learning_rate=0.1, n_epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the RBM are accessibles via a slicing \n",
    "print(dbn[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate images\n",
    "generated_images = dbn.generate_image(n_samples=5, n_gibbs_steps=1)\n",
    "\n",
    "# Display generated images\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(12, 4))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(generated_images[i].reshape(20, 16), cmap='gray')\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(DBN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int,\n",
    "        hidden_layer_sizes: List[int],\n",
    "        random_state=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Deep Neural Network (DNN).\n",
    "\n",
    "        Parameters:\n",
    "        - input_dim (int): Dimension of the input.\n",
    "        - output_dim (int): Dimension of the output.\n",
    "        - hidden_layer_sizes (List[int]): List of sizes for each hidden layer.\n",
    "        - random_state: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            n_visible=input_dim,\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        #--> self.rbms contains only the pre-trainable RBMs \n",
    "        self.clf = RBM(self.rbms[-1].n_hidden, output_dim)\n",
    "        self.network = self.rbms + [self.clf]  # DNN = [DBN + Classifier] ~ [RBM_0,...,RBM_N, RBM_Clf]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.network[key]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        join_repr = \"\\n\".join([f\"{'':4}{repr(rbm)},\" for rbm in self.network])\n",
    "        return f\"DNN([\\n{join_repr} <HEAD CLF>\\n])\"\n",
    "    \n",
    "    \n",
    "    def pretrain(self, n_epochs: int, learning_rate: float, batch_size: int, data: np.ndarray) -> \"DNN\":\n",
    "        \"\"\"\n",
    "        Pretrain the hidden layers of the DNN using the DBN training method.\n",
    "\n",
    "        Parameters:\n",
    "        - n_epochs (int): Number of training epochs.\n",
    "        - learning_rate (float): Learning rate for gradient descent.\n",
    "        - batch_size (int): Size of mini-batches.\n",
    "        - data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "\n",
    "        Returns:\n",
    "        - DNN: Pretrained DNN instance.\n",
    "        \"\"\"\n",
    "        # NOTE: Use the inherited `train` method to perform pre-training since `self.rbms`\n",
    "        # only contains the pre-trainable RBMs.\n",
    "        return self.train(data, n_epochs=n_epochs, learning_rate=learning_rate, batch_size=batch_size)\n",
    "    \n",
    "    def input_output(self, input_data: np.ndarray) -> Tuple[List[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        Get the outputs on each layer of the DNN and the softmax probabilities on the output layer.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "\n",
    "        Returns:\n",
    "        - Tuple[List[np.ndarray], np.ndarray]: Outputs on each layer & softmax probabilities.\n",
    "        \"\"\"\n",
    "        layer_outputs = []\n",
    "        \n",
    "        # Input layer output\n",
    "        layer_outputs.append(input_data)\n",
    "        \n",
    "        # Hidden layers output\n",
    "        for rbm in self.rbms:\n",
    "            layer_outputs.append(rbm.input_output(layer_outputs[-1]))\n",
    "        \n",
    "        # Softmax probabilities on the output layer\n",
    "        output_probs = self.network[-1].calcul_softmax(layer_outputs[-1])\n",
    "        \n",
    "        return layer_outputs, output_probs\n",
    "    \n",
    "\n",
    "    def _cross_entropy(batch_labels: np.ndarray, output_probs: np.ndarray, eps: float = 1e-15) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the cross entropy between the batch labels and output probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        - batch_labels (numpy.ndarray): True labels for the batch, shape (batch_size, n_classes).\n",
    "        - output_probs (numpy.ndarray): Predicted probabilities for the batch, shape (batch_size, n_classes).\n",
    "        - eps (float): Small value to avoid numerical instability in logarithm calculation. Default is 1e-15.\n",
    "\n",
    "        Returns:\n",
    "        - float: Cross entropy value.\n",
    "        \"\"\"\n",
    "        return -np.mean(np.sum(batch_labels * np.log(output_probs + eps), axis=1))\n",
    "\n",
    "\n",
    "    def backpropagation(\n",
    "            self,\n",
    "            input_data: np.ndarray,\n",
    "            labels: np.ndarray,\n",
    "            n_epochs: int,\n",
    "            learning_rate: float,\n",
    "            batch_size: int,\n",
    "            eps: float = 1e-15\n",
    "        ) -> \"DNN\":\n",
    "        \"\"\"\n",
    "        Estimate the weights/biases of the network using backpropagation algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - input_data (numpy.ndarray): Input data, shape (n_samples, n_visible).\n",
    "        - labels (numpy.ndarray): Labels for the input data, shape (n_samples, n_classes).\n",
    "        - n_epochs (int): Number of training epochs.\n",
    "        - learning_rate (float): Learning rate for gradient descent.\n",
    "        - batch_size (int): Size of mini-batches.\n",
    "        - eps (float): Small value to avoid numerical instability in logarithm calculation. Default is 1e-15.\n",
    "\n",
    "        Returns:\n",
    "        - DNN: Updated DNN instance.\n",
    "        \"\"\"\n",
    "        n_samples = input_data.shape[0]\n",
    "        \n",
    "        for epoch in tqdm(range(n_epochs), desc=\"Training\", unit=\"epoch\"):\n",
    "            for batch_start in range(0, n_samples, batch_size):\n",
    "                batch_end = min(batch_start + batch_size, n_samples)\n",
    "                batch_input = input_data[batch_start:batch_end]\n",
    "                batch_labels = labels[batch_start:batch_end]\n",
    "\n",
    "                # Forward pass\n",
    "                layer_outputs, output_probs = self.input_output(batch_input)\n",
    "\n",
    "                # Backward pass (update weights and biases)\n",
    "                self.network[-1].update(batch_labels, layer_outputs[-1], learning_rate)\n",
    "                for i in range(len(self.network) - 2, -1, -1):\n",
    "                    self.network[i].update(layer_outputs[i], layer_outputs[i + 1], self.network[i + 1].weights, learning_rate)\n",
    "\n",
    "            # Calculate cross entropy after each epoch\n",
    "            loss = self._cross_entropy(batch_labels, output_probs, eps)\n",
    "            tqdm.write(f\"Epoch {epoch + 1}/{n_epochs}, Cross Entropy: {loss}\")\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_visible=data.shape[1]\n",
    "hidden_layer_sizes = [100, 50, 25]\n",
    "output_dim = 20\n",
    "\n",
    "dnn = DNN(input_dim=n_visible, hidden_layer_sizes=hidden_layer_sizes, output_dim=output_dim, random_state=42)\n",
    "# keep last RBM's weights for further test.\n",
    "weights = dnn[-1].W \n",
    "\n",
    "dnn.train(data, learning_rate=0.1, n_epochs=10, batch_size=10)\n",
    "\n",
    "# Check that the last RBM has not been trained.\n",
    "np.testing.assert_equal (dnn[-1].a, 0) # visible bias\n",
    "np.testing.assert_equal (dnn[-1].b, 0) # hidden bias\n",
    "np.testing.assert_equal (dnn[-1].W, weights) # weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs, softmax_output = dnn.input_output(data)\n",
    "n_layers_net = len(layer_outputs) + 1\n",
    "print(\"No. network output =\", n_layers_net)\n",
    "\n",
    "print(f\"Input data (0): {layer_outputs[0].shape}\")\n",
    "for idx, layer_output in enumerate(layer_outputs[1:]):\n",
    "    print(f\"Hidden layer input ({idx+1}): {layer_output.shape}\")\n",
    "\n",
    "print(f\"Softmax output ({n_layers_net - 1}):\", softmax_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
